{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xray.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH7hZOK_kivb",
        "colab_type": "code",
        "outputId": "0587633f-ed51-40ce-b0a1-15631e3c9a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "import urllib\n",
        "\n",
        "# URLs for the zip files\n",
        "links = [\n",
        "    'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',\n",
        "    'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',\n",
        "    'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',\n",
        "\t'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',\n",
        "    'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',\n",
        "\t\n",
        "\t'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',\n",
        "\t'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',\n",
        "    'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',\n",
        "\t'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',\n",
        "\t'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',\n",
        "\t'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',\n",
        "\t'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'\n",
        "]\n",
        "\n",
        "for idx, link in enumerate(links):\n",
        "    fn = 'images_%02d.tar.gz' % (idx+1)\n",
        "    print('downloading '+fn+'...')\n",
        "    urllib.request.urlretrieve(link, fn)  # download the zip file\n",
        "print(\"completed\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading images_01.tar.gz...\n",
            "downloading images_02.tar.gz...\n",
            "downloading images_03.tar.gz...\n",
            "downloading images_04.tar.gz...\n",
            "downloading images_05.tar.gz...\n",
            "downloading images_06.tar.gz...\n",
            "downloading images_07.tar.gz...\n",
            "downloading images_08.tar.gz...\n",
            "downloading images_09.tar.gz...\n",
            "downloading images_10.tar.gz...\n",
            "downloading images_11.tar.gz...\n",
            "downloading images_12.tar.gz...\n",
            "completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE4csUjK_CbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf data/ sample_data/\n",
        "!mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woheD5oXWeZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xzf images_01.tar.gz -C data/ && rm images_01.tar.gz\n",
        "!tar -xzf images_02.tar.gz -C data/ && rm images_02.tar.gz\n",
        "!tar -xzf images_03.tar.gz -C data/ && rm images_03.tar.gz\n",
        "!tar -xzf images_04.tar.gz -C data/ && rm images_04.tar.gz\n",
        "!tar -xzf images_05.tar.gz -C data/ && rm images_05.tar.gz\n",
        "!tar -xzf images_06.tar.gz -C data/ && rm images_06.tar.gz\n",
        "!tar -xzf images_07.tar.gz -C data/ && rm images_07.tar.gz\n",
        "!tar -xzf images_08.tar.gz -C data/ && rm images_08.tar.gz\n",
        "!tar -xzf images_09.tar.gz -C data/ && rm images_09.tar.gz\n",
        "!tar -xzf images_10.tar.gz -C data/ && rm images_10.tar.gz\n",
        "!tar -xzf images_11.tar.gz -C data/ && rm images_11.tar.gz\n",
        "!tar -xzf images_12.tar.gz -C data/ && rm images_12.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc_GC4gyDsIC",
        "colab_type": "code",
        "outputId": "d4bb5001-a6cc-4e40-a288-3c3a4828bf31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install bcolz\n",
        "!pip install tensorflow==2.1.0\n",
        "!pip install autokeras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.17.5)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp36-cp36m-linux_x86_64.whl size=2663392 sha256=b3b20b7d3548a890e392f54c31c383498c0c22c88e62715ac0ea833eaeb6b521\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n",
            "Collecting tensorflow==2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.27.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.1.8)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 48.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.11.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.17.5)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (45.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.21.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting autokeras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/32/86ad97a008d73eb438e0efd153075a802c7a5d89eb5d6da8ee0812d3c544/autokeras-1.0.1-py3-none-any.whl (66kB)\n",
            "\r\u001b[K     |█████                           | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 30kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from autokeras) (1.17.5)\n",
            "Collecting keras-tuner>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.22.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from autokeras) (20.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from autokeras) (0.25.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->autokeras) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->autokeras) (0.8.6)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->autokeras) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->autokeras) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner>=1.0.1->autokeras) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->autokeras) (0.14.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->autokeras) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->autokeras) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->autokeras) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->autokeras) (2.6.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->autokeras) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->autokeras) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->autokeras) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner>=1.0.1->autokeras) (3.0.4)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp36-none-any.whl size=73200 sha256=b695f7871ebdc839ec3bf48546f339e57462cf59a5163e3d6360209b6f8e0950\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=e6093dbc57cd58a94b0ea725b6597a692df747193138b9d7e6cd2a1d31047a4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner, autokeras\n",
            "Successfully installed autokeras-1.0.1 colorama-0.4.3 keras-tuner-1.0.1 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uAvC22fDcFb",
        "colab_type": "code",
        "outputId": "8ebb9910-0573-4c4b-dfba-18c5bd4e65da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import random\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "img_path=\"data/images/\"\n",
        "img_height=224 #299\n",
        "img_width=224 #299\n",
        "num_classes = 15 #2\n",
        "\n",
        "# credit: https://github.com/fastai/fastai/blob/9e9ffbd49eb6490bb1168ce2ff32b10a81498ba9/fastai/utils.py\n",
        "import bcolz\n",
        "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
        "def load_array(fname): return bcolz.open(fname)[:]\n",
        "\n",
        "\n",
        "def get_image(img,display=False):\n",
        "    # if 'img' is a NumPy array, then it has already been loaded; just show it\n",
        "    if type(img).__module__ == np.__name__:\n",
        "        return img\n",
        "    else:\n",
        "        image = load_img(img, target_size=(img_width, img_height)) #, grayscale=True)\n",
        "        #image = Image.open(img_path).convert(\"L\")\n",
        "        #image = image.resize((img_height,img_width), Image.ANTIALIAS)\n",
        "        return np.asarray(image)\n",
        "    \n",
        "def get_image_batch(img,display=False):\n",
        "    image_arr = get_image(img,display)\n",
        "    if display == True:\n",
        "        plt.figure()\n",
        "        plt.imshow(image_arr)\n",
        "        plt.show()\n",
        "    return np.expand_dims(image_arr,axis=0)\n",
        "\n",
        "def plot_img(img, title, count, cols, plot_axis=False):\n",
        "    a = fig.add_subplot(1, cols, count)\n",
        "    # if 'img' is a NumPy array, then it has already been loaded; just show it\n",
        "    if type(img).__module__ == np.__name__:\n",
        "        plt.imshow(img)\n",
        "    else:\n",
        "        plt.imshow(load_img(img))\n",
        "    a.set_title(title,fontsize=10)\n",
        "    if plot_axis is False:\n",
        "        plt.axis('off')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-H_JAQiEFMx",
        "colab_type": "code",
        "outputId": "002540c2-6bf4-4f90-8b4d-30590ec97f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import csv\n",
        "\n",
        "def create_label_directories(csv_filename, img_path):\n",
        "    directories = set()\n",
        "    with open(csv_filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(csvfile) # skip header row\n",
        "        for row in reader:\n",
        "            img_filename = str(row[0])\n",
        "            labels = str(row[1])\n",
        "            for label in labels.split('|'):\n",
        "                #if label == \"Hernia\":\n",
        "                    #continue\n",
        "                src_file = os.path.join(img_path,img_filename)\n",
        "                label = \"_\".join(label.split())\n",
        "                dst_train_dir = os.path.join(img_path,\"train\",label)\n",
        "                dst_train_file = os.path.join(dst_train_dir,img_filename)\n",
        "                dst_valid_dir = os.path.join(img_path,\"valid\",label)\n",
        "                dst_test_dir = os.path.join(img_path,\"test\",label)\n",
        "                try:\n",
        "                    if not os.path.exists(dst_train_dir):\n",
        "                        os.makedirs(dst_train_dir)\n",
        "                        directories.add(label)\n",
        "                    if not os.path.exists(dst_valid_dir):\n",
        "                        os.makedirs(dst_valid_dir)\n",
        "                    if not os.path.exists(dst_test_dir):\n",
        "                        os.makedirs(dst_test_dir)\n",
        "                    src_file_abs = os.path.join(os.getcwd(),src_file)\n",
        "                    dst_train_file_abs = os.path.join(os.getcwd(),dst_train_file)\n",
        "                    #print(\"copy: \" + src_file_abs + \" to: \" + dst_train_file_abs)\n",
        "                    os.symlink(src_file_abs, dst_train_file_abs)\n",
        "                except FileNotFoundError:\n",
        "                    print(\"FileNotFoundError: \" + src_file)\n",
        "                else:\n",
        "                    directories.add(label)\n",
        "    return list(directories)\n",
        "\n",
        "print(img_path)\n",
        "directories = create_label_directories(\"data/Data_Entry_2017.csv\", img_path)\n",
        "print(directories)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/images/\n",
            "['Mass', 'Edema', 'Nodule', 'Pneumonia', 'Effusion', 'Pneumothorax', 'Fibrosis', 'Consolidation', 'Pleural_Thickening', 'Emphysema', 'Atelectasis', 'Hernia', 'Infiltration', 'No_Finding', 'Cardiomegaly']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwy9MCUcExm_",
        "colab_type": "code",
        "outputId": "6e2d7dbb-cb19-4ccf-8a6c-3af414127ab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def get_per_label_count(directories):\n",
        "    per_label_count = []\n",
        "    for ii in range(len(directories)):\n",
        "        path, dirs, files = os.walk(os.path.join(img_path,\"train\",directories[ii])).__next__()\n",
        "        file_count = len(files)\n",
        "        per_label_count.append(file_count)\n",
        "    return per_label_count\n",
        "        \n",
        "print(directories)\n",
        "per_label_count = get_per_label_count(directories) \n",
        "print(per_label_count)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mass', 'Edema', 'Nodule', 'Pneumonia', 'Effusion', 'Pneumothorax', 'Fibrosis', 'Consolidation', 'Pleural_Thickening', 'Emphysema', 'Atelectasis', 'Hernia', 'Infiltration', 'No_Finding', 'Cardiomegaly']\n",
            "[5782, 2303, 6331, 1431, 13317, 5302, 1686, 4667, 3385, 2516, 11559, 227, 19894, 60361, 2776]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H4zt70ZEy9l",
        "colab_type": "code",
        "outputId": "72028945-9410-4cb6-c347-ed9c2f7a73a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import shutil\n",
        "import numpy as np\n",
        "def split_train_valid_test(directories, per_label_count, valid_pct, test_pct):\n",
        "    for ii in range(len(directories)):\n",
        "        all_img_paths = glob.glob(os.path.join(img_path,\"train\",directories[ii],\"*.*\"))\n",
        "        np.random.shuffle(all_img_paths)\n",
        "        label_count = per_label_count[ii]\n",
        "        valid_count = int(label_count*valid_pct)\n",
        "        valid_files = all_img_paths[:valid_count]\n",
        "        all_img_paths[:valid_count] = []\n",
        "        test_count = int(label_count*test_pct)\n",
        "        test_files = all_img_paths[:test_count]\n",
        "        all_img_paths[:test_count] = []\n",
        "        #print(len(valid_files))\n",
        "        #print(len(test_files))\n",
        "        train_files = all_img_paths\n",
        "        all_img_paths = []\n",
        "        #print(len(train_files))\n",
        "        for valid_file in valid_files:\n",
        "            valid_file_abs = os.path.join(os.getcwd(),valid_file)\n",
        "            #print(\"move: '\" + valid_file_abs + \"' to: '\" + os.path.join(img_path,\"valid\",directories[ii]))\n",
        "            shutil.move(valid_file_abs, os.path.join(img_path,\"valid\",directories[ii]))\n",
        "        for test_file in test_files:\n",
        "            test_file_abs = os.path.join(os.getcwd(),test_file)\n",
        "            #print(\"move: '\" + test_file_abs + \"' to: '\" + os.path.join(img_path,\"test\",directories[ii]))\n",
        "            shutil.move(test_file_abs, os.path.join(img_path,\"test\",directories[ii]))\n",
        "        \n",
        "print(directories)\n",
        "print(per_label_count)\n",
        "valid_pct = 0.1\n",
        "test_pct = 0.1       \n",
        "split_train_valid_test(directories, per_label_count, valid_pct, test_pct)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Mass', 'Edema', 'Nodule', 'Pneumonia', 'Effusion', 'Pneumothorax', 'Fibrosis', 'Consolidation', 'Pleural_Thickening', 'Emphysema', 'Atelectasis', 'Hernia', 'Infiltration', 'No_Finding', 'Cardiomegaly']\n",
            "[5782, 2303, 6331, 1431, 13317, 5302, 1686, 4667, 3385, 2516, 11559, 227, 19894, 60361, 2776]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw5xAioPE64d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tuberculosis_class(img_path):\n",
        "    finding = img_path.split('.')[0][-3] #[-1]\n",
        "    if finding == '1':\n",
        "        return [0,1]\n",
        "    else:\n",
        "        return [1,0]\n",
        "\n",
        "def get_nih_class(img_path):\n",
        "    class_dir = img_path.split('/')[3] #[2]\n",
        "    if class_dir == 'No Finding' or class_dir == 'No_Finding':\n",
        "        return [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]\n",
        "    elif class_dir == 'Atelectasis':\n",
        "        return [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Cardiomegaly':\n",
        "        return [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Consolidation':\n",
        "        return [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Edema':\n",
        "        return [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Effusion':\n",
        "        return [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Emphysema':\n",
        "        return [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Fibrosis':\n",
        "        return [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Hernia':\n",
        "        return [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Infiltration':\n",
        "        return [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]\n",
        "    elif class_dir == 'Mass':\n",
        "        return [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]\n",
        "    elif class_dir == 'Nodule':\n",
        "        return [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]\n",
        "    elif class_dir == 'Pleural_Thickening':\n",
        "        return [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]\n",
        "    elif class_dir == 'Pneumonia':\n",
        "        return [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]\n",
        "    elif class_dir == 'Pneumothorax':\n",
        "        return [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]\n",
        "    else:\n",
        "        print(\"Unknown class: \" + class_dir)\n",
        "        return [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "def get_class(img_path, num_classes):\n",
        "    if num_classes == 2:\n",
        "        return get_tuberculosis_class(img_path)\n",
        "    elif num_classes == 15:\n",
        "        return get_nih_class(img_path)\n",
        "    else:\n",
        "        return []\n",
        "    \n",
        "def get_data(images_dir, num_classes):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    all_img_paths = glob.glob(os.path.join(images_dir, '*/*.*'))\n",
        "    np.random.shuffle(all_img_paths)\n",
        "    for img_path in all_img_paths:\n",
        "        imgs.append(get_image(img_path))\n",
        "        labels.append(get_class(img_path, num_classes))\n",
        "    return np.array(imgs), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIV5qJlbFZPW",
        "colab_type": "code",
        "outputId": "b38b225b-5660-47a1-8e49-b7fbfe9a897a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255#,\n",
        "    #shear_range=0.2,\n",
        "    #zoom_range=0.2,\n",
        "    #horizontal_flip=True, \n",
        "    #preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255#,\n",
        "    #preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(img_path,'train'),\n",
        "    target_size=(img_height,img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')#,\n",
        "    #color_mode='grayscale')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(img_path,'valid'),\n",
        "    target_size=(img_height,img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')#,\n",
        "    #color_mode='grayscale')\n",
        "    \n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(img_path,'test'),\n",
        "    target_size=(img_height,img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 113243 images belonging to 15 classes.\n",
            "Found 14147 images belonging to 15 classes.\n",
            "Found 14147 images belonging to 15 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6YtdxfrGr-4",
        "colab_type": "code",
        "outputId": "5ccc65d8-d0b2-48c1-b0c9-f809b878f776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from tensorflow.keras import optimizers, metrics\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, AveragePooling2D, GlobalAveragePooling2D\n",
        "\n",
        "def create_top_layer(model, num_classes, freeze_layers=True, display_summary=False):\n",
        "    if freeze_layers is True:\n",
        "        for layer in model.layers:\n",
        "            layer.trainable = False\n",
        "    top_model = Sequential()\n",
        "    top_model.add(GlobalAveragePooling2D(input_shape=model.output_shape[1:]))\n",
        "    top_model.add(Dense(num_classes))\n",
        "    top_model.add(Activation('softmax'))\n",
        "    if display_summary is True:\n",
        "        top_model.summary()\n",
        "    model = Model(inputs=model.input, outputs=top_model(model.output))\n",
        "    return model\n",
        "  \n",
        "model_imagenet_topless = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(img_width,img_height,3))\n",
        "model_imagenet = create_top_layer(model_imagenet_topless,num_classes)\n",
        "lr=0.001 #0.001 #1e-4 #0.1\n",
        "#mo=0.9\n",
        "decay=0.0\n",
        "model_imagenet.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=lr, decay=decay), metrics=['accuracy', metrics.top_k_categorical_accuracy])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6FzwFGVAB4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "filepath=\"/content/drive/My Drive/Projects/weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=batch_size)\n",
        "callbacks_list = [checkpoint,early_stopping]\n",
        "nb_train_samples = 113243\n",
        "nb_validation_samples= 14147  \n",
        "epochs = int(nb_train_samples/batch_size)*2 #1\n",
        "history = model_imagenet.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=batch_size, #nb_train_samples/batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=batch_size, #nb_validation_samples/val_batch_size,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMRJhoj2-qfG",
        "colab_type": "code",
        "outputId": "2d00e7a2-fa89-4430-b3d8-a363fc6db55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "source": [
        "# nasnet\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "img_path=\"./data/images\"\n",
        "num_classes=15\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255#,\n",
        "    #shear_range=0.2,\n",
        "    #zoom_range=0.2,\n",
        "    #horizontal_flip=True, \n",
        "    #preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255#,\n",
        "    #preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(img_path,'train'),\n",
        "    target_size=(331,331),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')#,\n",
        "    #color_mode='grayscale')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(img_path,'valid'),\n",
        "    target_size=(331,331),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')#,\n",
        "    #color_mode='grayscale')\n",
        "    \n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    os.path.join(img_path,'test'),\n",
        "    target_size=(331,331),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical')\n",
        "\n",
        "\n",
        "from tensorflow.keras import optimizers, metrics\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, AveragePooling2D, GlobalAveragePooling2D\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "def create_top_layer(model, num_classes, freeze_layers=True, display_summary=False):\n",
        "    if freeze_layers is True:\n",
        "        for layer in model.layers:\n",
        "            layer.trainable = False\n",
        "    top_model = Sequential()\n",
        "    top_model.add(GlobalAveragePooling2D(input_shape=model.output_shape[1:]))\n",
        "    top_model.add(Dense(num_classes))\n",
        "    top_model.add(Activation('softmax'))\n",
        "    if display_summary is True:\n",
        "        top_model.summary()\n",
        "    model = Model(inputs=model.input, outputs=top_model(model.output))\n",
        "    return model\n",
        "\n",
        "model_nas_topless = keras.applications.nasnet.NASNetLarge(input_shape=(331,331,3), include_top=False, weights='None', input_tensor=None, pooling='max', classes=15)\n",
        "model_nas = create_top_layer(model_nas_topless,num_classes)\n",
        "lr=0.001 #0.001 #1e-4 #0.1\n",
        "#mo=0.9\n",
        "decay=0.0\n",
        "model_nas.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=lr, decay=decay), metrics=['accuracy', metrics.top_k_categorical_accuracy])\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "\n",
        "filepath=\"/content/drive/My Drive/Projects/weights_nas_2.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #mode='max')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=batch_size)\n",
        "callbacks_list = [checkpoint,early_stopping]\n",
        "nb_train_samples = 113243\n",
        "nb_validation_samples= 14147\n",
        "epochs = int(nb_train_samples/batch_size)*2 #1\n",
        "history = model_nas.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=batch_size, #nb_train_samples/batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=batch_size, #nb_validation_samples/val_batch_size,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2f2a4f178fbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m331\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m331\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     class_mode='categorical')#,\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m#color_mode='grayscale')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mfollow_links\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_links\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         )\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/images/train'"
          ]
        }
      ]
    }
  ]
}