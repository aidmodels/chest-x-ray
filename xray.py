# -*- coding: utf-8 -*-
"""xray.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gFQWP29vtHNdJo5b_-HTIn60INnb_xWu
"""

import urllib

# URLs for the zip files
links = [
    'https://nihcc.box.com/shared/static/vfk49d74nhbxq3nqjg0900w5nvkorp5c.gz',
    'https://nihcc.box.com/shared/static/i28rlmbvmfjbl8p2n3ril0pptcmcu9d1.gz',
    'https://nihcc.box.com/shared/static/f1t00wrtdk94satdfb9olcolqx20z2jp.gz',
	'https://nihcc.box.com/shared/static/0aowwzs5lhjrceb3qp67ahp0rd1l1etg.gz',
    'https://nihcc.box.com/shared/static/v5e3goj22zr6h8tzualxfsqlqaygfbsn.gz',
	
	'https://nihcc.box.com/shared/static/asi7ikud9jwnkrnkj99jnpfkjdes7l6l.gz',
	'https://nihcc.box.com/shared/static/jn1b4mw4n6lnh74ovmcjb8y48h8xj07n.gz',
    'https://nihcc.box.com/shared/static/tvpxmn7qyrgl0w8wfh9kqfjskv6nmm1j.gz',
	'https://nihcc.box.com/shared/static/upyy3ml7qdumlgk2rfcvlb9k6gvqq2pj.gz',
	'https://nihcc.box.com/shared/static/l6nilvfa9cg3s28tqv1qc1olm3gnz54p.gz',
	'https://nihcc.box.com/shared/static/hhq8fkdgvcari67vfhs7ppg2w6ni4jze.gz',
	'https://nihcc.box.com/shared/static/ioqwiy20ihqwyr8pf4c24eazhh281pbu.gz'
]

for idx, link in enumerate(links):
    fn = 'images_%02d.tar.gz' % (idx+1)
    print('downloading '+fn+'...')
    urllib.request.urlretrieve(link, fn)  # download the zip file
print("completed")

!rm -rf data/ sample_data/
!mkdir data

!tar -xzf images_01.tar.gz -C data/ && rm images_01.tar.gz
!tar -xzf images_02.tar.gz -C data/ && rm images_02.tar.gz
!tar -xzf images_03.tar.gz -C data/ && rm images_03.tar.gz
!tar -xzf images_04.tar.gz -C data/ && rm images_04.tar.gz
!tar -xzf images_05.tar.gz -C data/ && rm images_05.tar.gz
!tar -xzf images_06.tar.gz -C data/ && rm images_06.tar.gz
!tar -xzf images_07.tar.gz -C data/ && rm images_07.tar.gz
!tar -xzf images_08.tar.gz -C data/ && rm images_08.tar.gz
!tar -xzf images_09.tar.gz -C data/ && rm images_09.tar.gz
!tar -xzf images_10.tar.gz -C data/ && rm images_10.tar.gz
!tar -xzf images_11.tar.gz -C data/ && rm images_11.tar.gz
!tar -xzf images_12.tar.gz -C data/ && rm images_12.tar.gz

!pip install bcolz
!pip install tensorflow==2.1.0
!pip install autokeras

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot as plt
# %matplotlib inline
import os
import glob
import math
import random
from keras.preprocessing.image import load_img, img_to_array

img_path="data/images/"
img_height=224 #299
img_width=224 #299
num_classes = 15 #2

# credit: https://github.com/fastai/fastai/blob/9e9ffbd49eb6490bb1168ce2ff32b10a81498ba9/fastai/utils.py
import bcolz
def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()
def load_array(fname): return bcolz.open(fname)[:]


def get_image(img,display=False):
    # if 'img' is a NumPy array, then it has already been loaded; just show it
    if type(img).__module__ == np.__name__:
        return img
    else:
        image = load_img(img, target_size=(img_width, img_height)) #, grayscale=True)
        #image = Image.open(img_path).convert("L")
        #image = image.resize((img_height,img_width), Image.ANTIALIAS)
        return np.asarray(image)
    
def get_image_batch(img,display=False):
    image_arr = get_image(img,display)
    if display == True:
        plt.figure()
        plt.imshow(image_arr)
        plt.show()
    return np.expand_dims(image_arr,axis=0)

def plot_img(img, title, count, cols, plot_axis=False):
    a = fig.add_subplot(1, cols, count)
    # if 'img' is a NumPy array, then it has already been loaded; just show it
    if type(img).__module__ == np.__name__:
        plt.imshow(img)
    else:
        plt.imshow(load_img(img))
    a.set_title(title,fontsize=10)
    if plot_axis is False:
        plt.axis('off')

import csv

def create_label_directories(csv_filename, img_path):
    directories = set()
    with open(csv_filename, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')
        next(csvfile) # skip header row
        for row in reader:
            img_filename = str(row[0])
            labels = str(row[1])
            for label in labels.split('|'):
                #if label == "Hernia":
                    #continue
                src_file = os.path.join(img_path,img_filename)
                label = "_".join(label.split())
                dst_train_dir = os.path.join(img_path,"train",label)
                dst_train_file = os.path.join(dst_train_dir,img_filename)
                dst_valid_dir = os.path.join(img_path,"valid",label)
                dst_test_dir = os.path.join(img_path,"test",label)
                try:
                    if not os.path.exists(dst_train_dir):
                        os.makedirs(dst_train_dir)
                        directories.add(label)
                    if not os.path.exists(dst_valid_dir):
                        os.makedirs(dst_valid_dir)
                    if not os.path.exists(dst_test_dir):
                        os.makedirs(dst_test_dir)
                    src_file_abs = os.path.join(os.getcwd(),src_file)
                    dst_train_file_abs = os.path.join(os.getcwd(),dst_train_file)
                    #print("copy: " + src_file_abs + " to: " + dst_train_file_abs)
                    os.symlink(src_file_abs, dst_train_file_abs)
                except FileNotFoundError:
                    print("FileNotFoundError: " + src_file)
                else:
                    directories.add(label)
    return list(directories)

print(img_path)
directories = create_label_directories("data/Data_Entry_2017.csv", img_path)
print(directories)

def get_per_label_count(directories):
    per_label_count = []
    for ii in range(len(directories)):
        path, dirs, files = os.walk(os.path.join(img_path,"train",directories[ii])).__next__()
        file_count = len(files)
        per_label_count.append(file_count)
    return per_label_count
        
print(directories)
per_label_count = get_per_label_count(directories) 
print(per_label_count)

import shutil
import numpy as np
def split_train_valid_test(directories, per_label_count, valid_pct, test_pct):
    for ii in range(len(directories)):
        all_img_paths = glob.glob(os.path.join(img_path,"train",directories[ii],"*.*"))
        np.random.shuffle(all_img_paths)
        label_count = per_label_count[ii]
        valid_count = int(label_count*valid_pct)
        valid_files = all_img_paths[:valid_count]
        all_img_paths[:valid_count] = []
        test_count = int(label_count*test_pct)
        test_files = all_img_paths[:test_count]
        all_img_paths[:test_count] = []
        #print(len(valid_files))
        #print(len(test_files))
        train_files = all_img_paths
        all_img_paths = []
        #print(len(train_files))
        for valid_file in valid_files:
            valid_file_abs = os.path.join(os.getcwd(),valid_file)
            #print("move: '" + valid_file_abs + "' to: '" + os.path.join(img_path,"valid",directories[ii]))
            shutil.move(valid_file_abs, os.path.join(img_path,"valid",directories[ii]))
        for test_file in test_files:
            test_file_abs = os.path.join(os.getcwd(),test_file)
            #print("move: '" + test_file_abs + "' to: '" + os.path.join(img_path,"test",directories[ii]))
            shutil.move(test_file_abs, os.path.join(img_path,"test",directories[ii]))
        
print(directories)
print(per_label_count)
valid_pct = 0.1
test_pct = 0.1       
split_train_valid_test(directories, per_label_count, valid_pct, test_pct)

def get_tuberculosis_class(img_path):
    finding = img_path.split('.')[0][-3] #[-1]
    if finding == '1':
        return [0,1]
    else:
        return [1,0]

def get_nih_class(img_path):
    class_dir = img_path.split('/')[3] #[2]
    if class_dir == 'No Finding' or class_dir == 'No_Finding':
        return [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]
    elif class_dir == 'Atelectasis':
        return [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
    elif class_dir == 'Cardiomegaly':
        return [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]
    elif class_dir == 'Consolidation':
        return [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]
    elif class_dir == 'Edema':
        return [0,0,0,1,0,0,0,0,0,0,0,0,0,0,0]
    elif class_dir == 'Effusion':
        return [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]
    elif class_dir == 'Emphysema':
        return [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0]
    elif class_dir == 'Fibrosis':
        return [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0]
    elif class_dir == 'Hernia':
        return [0,0,0,0,0,0,0,1,0,0,0,0,0,0,0]
    elif class_dir == 'Infiltration':
        return [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]
    elif class_dir == 'Mass':
        return [0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]
    elif class_dir == 'Nodule':
        return [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]
    elif class_dir == 'Pleural_Thickening':
        return [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0]
    elif class_dir == 'Pneumonia':
        return [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0]
    elif class_dir == 'Pneumothorax':
        return [0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]
    else:
        print("Unknown class: " + class_dir)
        return [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]

def get_class(img_path, num_classes):
    if num_classes == 2:
        return get_tuberculosis_class(img_path)
    elif num_classes == 15:
        return get_nih_class(img_path)
    else:
        return []
    
def get_data(images_dir, num_classes):
    imgs = []
    labels = []
    all_img_paths = glob.glob(os.path.join(images_dir, '*/*.*'))
    np.random.shuffle(all_img_paths)
    for img_path in all_img_paths:
        imgs.append(get_image(img_path))
        labels.append(get_class(img_path, num_classes))
    return np.array(imgs), np.array(labels)

from keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rescale=1./255#,
    #shear_range=0.2,
    #zoom_range=0.2,
    #horizontal_flip=True, 
    #preprocessing_function=preprocess_input
)

test_datagen = ImageDataGenerator(
    rescale=1./255#,
    #preprocessing_function=preprocess_input
)

batch_size = 32

train_generator = train_datagen.flow_from_directory(
    os.path.join(img_path,'train'),
    target_size=(img_height,img_width),
    batch_size=batch_size,
    class_mode='categorical')#,
    #color_mode='grayscale')

validation_generator = test_datagen.flow_from_directory(
    os.path.join(img_path,'valid'),
    target_size=(img_height,img_width),
    batch_size=batch_size,
    class_mode='categorical')#,
    #color_mode='grayscale')
    
test_generator = test_datagen.flow_from_directory(
    os.path.join(img_path,'test'),
    target_size=(img_height,img_width),
    batch_size=batch_size,
    class_mode='categorical')

from tensorflow.keras import optimizers, metrics
from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, AveragePooling2D, GlobalAveragePooling2D

def create_top_layer(model, num_classes, freeze_layers=True, display_summary=False):
    if freeze_layers is True:
        for layer in model.layers:
            layer.trainable = False
    top_model = Sequential()
    top_model.add(GlobalAveragePooling2D(input_shape=model.output_shape[1:]))
    top_model.add(Dense(num_classes))
    top_model.add(Activation('softmax'))
    if display_summary is True:
        top_model.summary()
    model = Model(inputs=model.input, outputs=top_model(model.output))
    return model
  
model_imagenet_topless = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(img_width,img_height,3))
model_imagenet = create_top_layer(model_imagenet_topless,num_classes)
lr=0.001 #0.001 #1e-4 #0.1
#mo=0.9
decay=0.0
model_imagenet.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=lr, decay=decay), metrics=['accuracy', metrics.top_k_categorical_accuracy])

from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard
filepath="/content/drive/My Drive/Projects/weights.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #mode='max')
early_stopping = EarlyStopping(monitor='val_loss', patience=batch_size)
callbacks_list = [checkpoint,early_stopping]
nb_train_samples = 113243
nb_validation_samples= 14147  
epochs = int(nb_train_samples/batch_size)*2 #1
history = model_imagenet.fit(
    train_generator,
    steps_per_epoch=batch_size, #nb_train_samples/batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=batch_size, #nb_validation_samples/val_batch_size,
    callbacks=callbacks_list,
    verbose=1)

# nasnet
from keras.preprocessing.image import ImageDataGenerator
import os
img_path="./data/images"
num_classes=15

train_datagen = ImageDataGenerator(
    rescale=1./255#,
    #shear_range=0.2,
    #zoom_range=0.2,
    #horizontal_flip=True, 
    #preprocessing_function=preprocess_input
)

test_datagen = ImageDataGenerator(
    rescale=1./255#,
    #preprocessing_function=preprocess_input
)

batch_size = 128

train_generator = train_datagen.flow_from_directory(
    os.path.join(img_path,'train'),
    target_size=(331,331),
    batch_size=batch_size,
    class_mode='categorical')#,
    #color_mode='grayscale')

validation_generator = test_datagen.flow_from_directory(
    os.path.join(img_path,'valid'),
    target_size=(331,331),
    batch_size=batch_size,
    class_mode='categorical')#,
    #color_mode='grayscale')
    
test_generator = test_datagen.flow_from_directory(
    os.path.join(img_path,'test'),
    target_size=(331,331),
    batch_size=batch_size,
    class_mode='categorical')


from tensorflow.keras import optimizers, metrics
from tensorflow.keras.applications import InceptionResNetV2
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, AveragePooling2D, GlobalAveragePooling2D
import tensorflow.keras as keras

def create_top_layer(model, num_classes, freeze_layers=True, display_summary=False):
    if freeze_layers is True:
        for layer in model.layers:
            layer.trainable = False
    top_model = Sequential()
    top_model.add(GlobalAveragePooling2D(input_shape=model.output_shape[1:]))
    top_model.add(Dense(num_classes))
    top_model.add(Activation('softmax'))
    if display_summary is True:
        top_model.summary()
    model = Model(inputs=model.input, outputs=top_model(model.output))
    return model

model_nas_topless = keras.applications.nasnet.NASNetLarge(input_shape=(331,331,3), include_top=False, weights=None, input_tensor=None, pooling=None, classes=15)
model_nas = create_top_layer(model_nas_topless,num_classes)
lr=0.001 #0.001 #1e-4 #0.1
#mo=0.9
decay=0.0
model_nas.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=lr, decay=decay), metrics=['accuracy', metrics.top_k_categorical_accuracy])
from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard

filepath="/content/drive/My Drive/Projects/weights_nas_2.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min') #mode='max')
early_stopping = EarlyStopping(monitor='val_loss', patience=batch_size)
callbacks_list = [checkpoint,early_stopping]
nb_train_samples = 113243
nb_validation_samples= 14147
epochs = int(nb_train_samples/batch_size)*2 #1
history = model_nas.fit(
    train_generator,
    steps_per_epoch=batch_size, #nb_train_samples/batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=batch_size, #nb_validation_samples/val_batch_size,
    callbacks=callbacks_list,
    verbose=1)